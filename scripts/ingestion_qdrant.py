import sys
from pathlib import Path

from llama_index.core import Document, StorageContext, VectorStoreIndex
from llama_index.core.node_parser import HierarchicalNodeParser
from llama_index.readers.file import PDFReader
from llama_index.vector_stores.qdrant import QdrantVectorStore

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH (–æ–¥–Ω–æ—Ä–∞–∑–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω)
root_path = Path(__file__).parent.parent  # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Å–∫—Ä–∏–ø—Ç –≤ scripts/
sys.path.append(str(root_path))

from app.core.database import client
from app.core.ml_models import embed_model
from app.utils.validators import ensure_path_exists


class IngestionPipeline:

    def load_pdf(self, file_path: Path | str) -> list[Document]:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º PDF –ø–æ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É"""
        try:

            loc_dir = ensure_path_exists(file_path)
            print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º PDF from: {loc_dir}")
            reader = PDFReader()
            documents = reader.load_data(file=loc_dir)
            print(f"‚úÖ –ó–∞–≥—Ä—É–∑–∏–ª–∏ {len(documents)} document(s) from PDF.")
            return documents

        except Exception as e:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: {e}")
            sys.exit(1)

    def chunk_documents(self, documents: list[Document]) -> list:
        """–†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–∞ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —á–∞–Ω–∫–∏ (–Ω–æ–¥—ã)."""
        try:

            print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º —á–∞–Ω–∫–æ–≤–∞—Ç—å")
            node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[1024, 512])
            nodes = node_parser.get_nodes_from_documents(documents)
            print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(nodes)} –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –Ω–æ–¥.")
            return nodes

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ –Ω–∞ —á–∞–Ω–∫–∏: {e}")
            sys.exit(1)

    def chunk2vDB(self, nodes: list, collection_name: str):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–æ–¥—ã –≤ Qdrant"""
        try:

            print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –ë–î...")

            vector_store = QdrantVectorStore(
                client=client,
                collection_name=collection_name,
                distance_metric="Cosine",
            )

            storage_context = StorageContext.from_defaults(vector_store=vector_store)

            VectorStoreIndex(
                nodes=nodes,
                storage_context=storage_context,
                embed_model=embed_model,
                show_progress=True,
            )
            print(
                f"‚úÖ –£—Å–ø–µ—à–Ω–æ wrote {len(nodes)} nodes to Qdrant collection '{collection_name}'."
            )

        except Exception as e:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤ qdrant: {e}")
            sys.exit(1)

    def run(self, file_path: Path | str, collection_name: str):
        """
        –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: –∑–∞–≥—Ä—É–∑–∫–∞ PDF ‚Üí —á–∞–Ω–∫–∏–Ω–≥ ‚Üí –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant
        :param file_path: –ü—É—Ç—å –∫ PDF-—Ñ–∞–π–ª—É
        :param collection_name: –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant
        """
        documents = self.load_pdf(file_path)
        nodes = self.chunk_documents(documents)
        self.chunk2vDB(nodes, collection_name)


if __name__ == "__main__":
    pdf_path = Path(__file__).parent / "datasets" / "main_datasets" / "polovoz.pdf"
    loc_dir = ensure_path_exists(pdf_path)

    collection_name = "math"

    print(f"üîÑ Loading PDF: {loc_dir.name}")
    loader = IngestionPipeline()
    loader.run(loc_dir, collection_name)
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ ingested into Qdrant collection '{collection_name}'")
